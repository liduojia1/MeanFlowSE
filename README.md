<div align="center"> <h1>MeanFlowSE ¬∑ One-Step Generative Speech Enhancement</h1> <p>   <a href="https://arxiv.org/abs/2509.14858">     <img alt="Paper" src="https://img.shields.io/badge/Paper-arXiv-b31b1b?logo=arxiv&logoColor=white">   </a>   <a href="https://huggingface.co/liduojia/MeanFlowSE">     <img alt="HF Model" src="https://img.shields.io/badge/Model-HuggingFace-yellow?logo=huggingface">   </a> </p>


</div>



**MeanFlowSE** is a conditional generative approach to speech enhancement. It learns **average velocities over short time spans** and performs enhancement with a **single backward-in-time displacement** (1-NFE), avoiding long ODE rollouts. The training objective is local (JVP-based) and **matches conditional flow matching on the diagonal (r = t)**‚Äîno teacher models, schedulers, or distillation required. In practice, 1-NFE inference makes real-time or near-real-time deployment straightforward on standard hardware.

![MeanFlowSE](MeanFlowSE.png)

------

## üéß Demos

- Online demo: **coming soon**
- See **üß∞ Pretrained Models** below for ready-to-use weights

------

## üóÇÔ∏è Table of Contents

- [‚ú® Highlights](#-highlights)
- [üîé What‚Äôs Inside](#-whats-inside)
- [‚ö° Quick Start](#-quick-start)
  - [Installation](#installation)
  - [Data Preparation](#data-preparation)
  - [Training](#training)
  - [Inference](#inference)
- [üõ†Ô∏è Configuration](#Ô∏è-configuration)
- [üèóÔ∏è Repository Structure](#Ô∏è-repository-structure)
- [üß∞ Pretrained Models](#-pretrained-models)
- [üìö Built Upon & Related Work](#-built-upon--related-work)
- [üôè Acknowledgments](#-acknowledgments)
- [üìù Citation](#-citation)

------

## ‚ú® Highlights

- **One-step enhancement (1-NFE):** A single **displacement** replaces long ODE trajectories‚Äîsuitable for real-time scenarios on CPUs/GPUs.
- **No teachers, no distillation:** Local JVP-based training; exactly matches conditional flow matching when r=t.
- **Two samplers, one model:**
  - `euler_mf` ‚Üí **average-field displacement** (one-step/few-step; recommended)
  - `euler` ‚Üí **instantaneous-field Euler** (multi-step fallback for ablations)
- **End-to-end front-end:** Complex STFT pipeline; metrics include **PESQ / ESTOI / SI-SDR / DNSMOS / RTF**.

------

## üîé What‚Äôs Inside

- **Training:** Supervision from the **average velocity field** (1-step displacement sampler), with JVP for stability; when r=t the objective reduces to standard conditional flow matching.
- **Inference:** `euler_mf` for one-step displacement; `euler` for multi-step Euler along the instantaneous field.
- **Audio front-end:** Complex STFT with configurable transforms and normalization.
- **Metrics:** PESQ, ESTOI, SI-SDR, DNSMOS, and end-to-end **RTF** measurement.

------

## ‚ö° Quick Start

### Installation

```
# Python 3.10 recommended
pip install -r requirements.txt
# Install a recent PyTorch + CUDA build compatible with your GPUs if you train multi-GPU
```

### Data Preparation

Expected layout (defaults assume 16 kHz, centered frames, Hann windows, complex STFT):

```
<BASE_DIR>/
  train/clean/*.wav
  train/noisy/*.wav
  valid/clean/*.wav
  valid/noisy/*.wav
  test/clean/*.wav
  test/noisy/*.wav
```


### Training

**Single machine, multi-GPU (DDP)**

```
# Edit DATA_DIR and GPU count inside the script if needed
bash scripts/train_vbd.sh
```

**Or run directly**

```
torchrun --standalone --nproc_per_node=4 train.py \
  --backbone ncsnpp \
  --ode flowmatching \
  --base_dir <BASE_DIR> \
  --batch_size 2 --num_workers 8 \
  --max_epochs 150 --precision 32 --gradient_clip_val 1.0 \
  --t_eps 0.03 --T_rev 1.0 \
  --sigma_min 0.0 --sigma_max 0.487 \
  --use_mfse \
  --mf_weight_final 0.25 --mf_warmup_frac 0.5 \
  --mf_delta_gamma_start 8.0 --mf_delta_gamma_end 1.0 \
  --mf_delta_warmup_frac 0.7 \
  --mf_r_equals_t_prob 0.1 \
  --mf_jvp_clip 5.0 --mf_jvp_eps 1e-3 \
  --mf_jvp_impl fd --mf_jvp_chunk 1 \
  --mf_skip_weight_thresh 0.05 \
  --val_metrics_every_n_epochs 1 \
  --default_root_dir lightning_logs
```

- Logs & checkpoints under `lightning_logs/<exp_name>/version_x/`.
- Heavy validation (PESQ/ESTOI/SI-SDR) runs **periodically on rank-0**; other ranks log placeholders so checkpoint monitors remain consistent.

### Inference

**Convenience script**

```
# MODE = multistep | multistep_mf | onestep
MODE=onestep STEPS=1 \
TEST_DATA_DIR=<BASE_DIR> \
CKPT_INPUT=path/to/best.ckpt \
bash run_inference.sh
```

**Or call the evaluator**

```
python evaluate.py \
  --test_dir <BASE_DIR> \
  --folder_destination /path/to/output \
  --ckpt path/to/best.ckpt \
  --odesolver euler_mf \
  --reverse_starting_point 1.0 \
  --last_eval_point 0.0 \
  --one_step
```

> `evaluate.py` writes **enhanced WAVs**.
>  If `--odesolver` is omitted, it **auto-selects** (`euler_mf` when MF-SE was used; otherwise `euler`).

------

## üõ†Ô∏è Configuration

Common flags to tweak:

- **Time & schedule** ‚Äî `--T_rev` (reverse start, default 1.0), `--t_eps` (terminal time), `--sigma_min`, `--sigma_max`
- **MF-SE stability** ‚Äî `--mf_jvp_impl {auto,fd,autograd}`, `--mf_jvp_chunk`, `--mf_jvp_clip`, `--mf_jvp_eps`; curriculum: `--mf_weight_final`, `--mf_warmup_frac`, `--mf_delta_*`, `--mf_r_equals_t_prob`
- **Validation cost** ‚Äî `--val_metrics_every_n_epochs`, `--num_eval_files`
- **Backbone & front-end** ‚Äî see `flowmse/backbones/` and `SpecsDataModule`

------

## üèóÔ∏è Repository Structure

```
MeanFlowSE/
‚îú‚îÄ‚îÄ train.py                  # Lightning entry point
‚îú‚îÄ‚îÄ evaluate.py               # Enhancement script (saves WAV)
‚îú‚îÄ‚îÄ run_inference.sh          # One-step / few-step convenience runner
‚îú‚îÄ‚îÄ flowmse/
‚îÇ   ‚îú‚îÄ‚îÄ model.py              # Losses, JVP, curriculum, logging
‚îÇ   ‚îú‚îÄ‚îÄ odes.py               # Path definition & registry
‚îÇ   ‚îú‚îÄ‚îÄ sampling/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ odesolvers.py     # Euler (instantaneous) & Euler-MF (displacement)
‚îÇ   ‚îú‚îÄ‚îÄ backbones/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ncsnpp.py         # U-Net with time/Œît embeddings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ data_module.py        # STFT I/O pipeline
‚îÇ   ‚îî‚îÄ‚îÄ util/                 # metrics, registry, tensors, inference helpers
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ scripts/
    ‚îî‚îÄ‚îÄ train_vbd.sh
```

------

## üß∞ Pretrained Models

- **VoiceBank‚ÄìDEMAND (16 kHz)** ‚Äî weights on Google Drive:
   üëâ [Download](https://drive.google.com/file/d/1QAxgd5BWrxiNi0q2qD3n1Xcv6bW0X86-/view?usp=sharing)

------

## üìö Built Upon & Related Work

This repository builds upon and is inspired by the following excellent works (front-end design, training/evaluation infrastructure, etc.):

- **SGMSE** ‚Äî https://github.com/sp-uhh/sgmse
- **SGMSE-CRP** ‚Äî https://github.com/sp-uhh/sgmse_crp
- **SGMSE-BBED** ‚Äî https://github.com/sp-uhh/sgmse-bbed
- **FLOWMSE (FlowSE)** ‚Äî https://github.com/seongq/flowmse

------

## üôè Acknowledgments

We gratefully acknowledge **Prof. Xie Chen‚Äôs group (X-LANCE Lab, SJTU)** for valuable guidance and engineering tips during training.

------

## üìù Citation

**Preprint**

```
@misc{li2025meanflowseonestepgenerativespeech,
  title         = {MeanFlowSE: one-step generative speech enhancement via conditional mean flow},
  author        = {Duojia Li and Shenghui Lu and Hongchen Pan and Zongyi Zhan and Qingyang Hong and Lin Li},
  year          = {2025},
  eprint        = {2509.14858},
  archivePrefix = {arXiv},
  primaryClass  = {cs.SD},
  url           = {https://arxiv.org/abs/2509.14858}
}
```

> **Status:** Submitted to **ICASSP 2026** (under review). We‚Äôll add the official link and updated BibTeX upon acceptance.
>
> **License:** This repository is released under the **MIT License**.

------

**Questions or issues?** Please open a GitHub issue or pull request. Contributions are welcome‚Äîfrom bug fixes to new backbones and front-ends.
